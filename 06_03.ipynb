{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying ViTs\n",
    "## Chapter 6 Module 3\n",
    "\n",
    "In this module, we will look at some common techniques on how to deploy transformers. Now, transformers can be very bulky so it helps to use methods like quanitzation to shrink the size of the model at deployment time. This will allow the model to run much faster without sacrificing too much accuracy. \n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torchvision optimum onnx onnxruntime --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from io import BytesIO\n",
    "import fiftyone as fo \n",
    "import fiftyone.zoo as foz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's kick things off by loading in our ViT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab our imagenet sample to test with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.delete_dataset(\"Imagenet-Sample\")\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"imagenet-sample\",\n",
    "    dataset_name=\"Imagenet-Sample\",\n",
    "    max_samples=50,\n",
    "    shuffle=True,\n",
    "    overwrite=True,\n",
    ")\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Quantization (and Why Does It Matter)?\n",
    "In deep learning, quantization is the process of converting a model’s weights and activations from high-precision floating point numbers (like float32) into lower-precision formats — such as int8 or float16.\n",
    "\n",
    "This sounds simple, but it has powerful benefits. Vision Transformers (ViTs) are large and memory-hungry. They often have millions of parameters, heavy matrix multiplications, and high VRAM or RAM usage. That makes them hard to deploy on mobile phones, edge devices, low-latency environments.\n",
    "\n",
    "Quanitzation gives you a smaller model, faster inference, and lower memory usage. All this combines for real world deployability.\n",
    "\n",
    "\n",
    "To start we need to check our support engines. Basically there are many different forms of quantization including post-training quantization which we will be doing and quantized aware training. Quantization can be a mixed bag at times, to get the best results, target exactly your hardware and use a good quantization method. \n",
    "\n",
    "We will be using a basic one from torch first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.quantized.supported_engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Explicitly enable a compatible quantization engine\n",
    "torch.backends.quantized.engine = \"qnnpack\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantize the model to `qint8`making it effectively 4x smaller in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that is left to do is to test it! Let's start with a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "\n",
    "image = Image.open(dataset.first().filepath)\n",
    "inputs = processor(images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the image\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = quantized_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = logits.argmax(-1).item()\n",
    "    print(\"Predicted class:\", model.config.id2label[predicted_class])\n",
    "    print(\"Actual class:\", dataset.first().ground_truth.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to confirm that are model got faster from quantization! Let's benchmark our model on 100 images next. `perf_counter` is great for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "def benchmark_model(model, inputs, num_iterations=100):\n",
    "    start_time = perf_counter()\n",
    "    for _ in range(num_iterations):\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    end_time = perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    avg_time = elapsed_time / num_iterations\n",
    "    return avg_time\n",
    "\n",
    "# Benchmark the normal model\n",
    "avg_time = benchmark_model(model, inputs)\n",
    "\n",
    "print(f\"Average inference time for normal model: {avg_time:.6f} seconds\")\n",
    "\n",
    "# Benchmark the quantized model\n",
    "avg_time = benchmark_model(quantized_model, inputs)\n",
    "\n",
    "print(f\"Average inference time for quantized model: {avg_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh! Did your quantized model not get faster? Don't worry, chances are then that your computer just does not support int8 acceleration and is just inferring it instead! To capture all of quantization, targeting a device with int8 support is ideal, like a Raspberry Pi, Nvidia GPUs, or Qualcomm Snapdragons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
