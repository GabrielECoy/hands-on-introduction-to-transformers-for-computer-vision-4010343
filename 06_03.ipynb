{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying ViTs\n",
    "## Chapter 6 Module 3\n",
    "\n",
    "In this module, we will look at some common techniques on how to deploy transformers. Now, transformers can be very bulky so it helps to use methods like quanitzation to shrink the size of the model at deployment time. This will allow the model to run much faster without sacrificing too much accuracy. \n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/.fiftyone-venv/lib/python3.11/site-packages (4.54.1)\n",
      "Requirement already satisfied: torchvision in /opt/.fiftyone-venv/lib/python3.11/site-packages (0.22.1)\n",
      "Requirement already satisfied: optimum in /opt/.fiftyone-venv/lib/python3.11/site-packages (1.25.3)\n",
      "Requirement already satisfied: onnx in /opt/.fiftyone-venv/lib/python3.11/site-packages (1.18.0)\n",
      "Requirement already satisfied: onnxruntime in /opt/.fiftyone-venv/lib/python3.11/site-packages (1.22.0)\n",
      "Requirement already satisfied: filelock in /opt/.fiftyone-venv/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/.fiftyone-venv/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: torch==2.7.1 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torchvision) (2.7.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from torch==2.7.1->torchvision) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from triton==3.3.1->torch==2.7.1->torchvision) (80.3.1)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from onnx) (6.31.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/.fiftyone-venv/lib/python3.11/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/.fiftyone-venv/lib/python3.11/site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from jinja2->torch==2.7.1->torchvision) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/.fiftyone-venv/lib/python3.11/site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torchvision optimum onnx onnxruntime #--quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.fiftyone-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from io import BytesIO\n",
    "import fiftyone as fo \n",
    "import fiftyone.zoo as foz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's kick things off by loading in our ViT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab our imagenet sample to test with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing directory '/fiftyone/zoo/datasets/imagenet-sample'\n",
      "Downloading dataset to '/fiftyone/zoo/datasets/imagenet-sample'\n",
      "Downloading dataset...\n",
      " 100% |████|  762.4Mb/762.4Mb [961.0ms elapsed, 0s remaining, 793.3Mb/s]      \n",
      "Extracting dataset...\n",
      "Parsing dataset metadata\n",
      "Found 1000 samples\n",
      "Dataset info written to '/fiftyone/zoo/datasets/imagenet-sample/info.json'\n",
      "Loading 'imagenet-sample'\n",
      " 100% |███████████████████| 50/50 [48.4ms elapsed, 0s remaining, 1.0K samples/s]   \n",
      "Dataset 'Imagenet-Sample' created\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://0.0.0.0:5151/?notebook=True&subscription=7b73e984-713c-47c6-824a-6f2123e5993c&polling=true\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7840bee84f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fo.delete_dataset(\"Imagenet-Sample\")\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"imagenet-sample\",\n",
    "    dataset_name=\"Imagenet-Sample\",\n",
    "    max_samples=50,\n",
    "    shuffle=True,\n",
    "    overwrite=True,\n",
    ")\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Quantization (and Why Does It Matter)?\n",
    "In deep learning, quantization is the process of converting a model’s weights and activations from high-precision floating point numbers (like float32) into lower-precision formats — such as int8 or float16.\n",
    "\n",
    "This sounds simple, but it has powerful benefits. Vision Transformers (ViTs) are large and memory-hungry. They often have millions of parameters, heavy matrix multiplications, and high VRAM or RAM usage. That makes them hard to deploy on mobile phones, edge devices, low-latency environments.\n",
    "\n",
    "Quanitzation gives you a smaller model, faster inference, and lower memory usage. All this combines for real world deployability.\n",
    "\n",
    "\n",
    "To start we need to check our support engines. Basically there are many different forms of quantization including post-training quantization which we will be doing and quantized aware training. Quantization can be a mixed bag at times, to get the best results, target exactly your hardware and use a good quantization method. \n",
    "\n",
    "We will be using a basic one from torch first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qnnpack', 'none', 'onednn', 'x86', 'fbgemm']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.quantized.supported_engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Explicitly enable a compatible quantization engine\n",
    "torch.backends.quantized.engine = \"qnnpack\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantize the model to `qint8`making it effectively 4x smaller in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that is left to do is to test it! Let's start with a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "\n",
    "image = Image.open(dataset.first().filepath)\n",
    "inputs = processor(images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W803 17:03:24.526202120 qlinear_dynamic.cpp:251] Warning: Currently, qnnpack incorrectly ignores reduce_range when it is set to true; this may change in a future release. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Chihuahua\n",
      "Actual class: Chihuahua\n"
     ]
    }
   ],
   "source": [
    "# Test the image\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = quantized_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = logits.argmax(-1).item()\n",
    "    print(\"Predicted class:\", model.config.id2label[predicted_class])\n",
    "    print(\"Actual class:\", dataset.first().ground_truth.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to confirm that are model got faster from quantization! Let's benchmark our model on 100 images next. `perf_counter` is great for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time for normal model: 0.459200 seconds\n",
      "Average inference time for quantized model: 0.945831 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "def benchmark_model(model, inputs, num_iterations=10): #100\n",
    "    start_time = perf_counter()\n",
    "    for _ in range(num_iterations):\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    end_time = perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    avg_time = elapsed_time / num_iterations\n",
    "    return avg_time\n",
    "\n",
    "# Benchmark the normal model\n",
    "avg_time = benchmark_model(model, inputs)\n",
    "\n",
    "print(f\"Average inference time for normal model: {avg_time:.6f} seconds\")\n",
    "\n",
    "# Benchmark the quantized model\n",
    "avg_time = benchmark_model(quantized_model, inputs)\n",
    "\n",
    "print(f\"Average inference time for quantized model: {avg_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh! Did your quantized model not get faster? Don't worry, chances are then that your computer just does not support int8 acceleration and is just inferring it instead! To capture all of quantization, targeting a device with int8 support is ideal, like a Raspberry Pi, Nvidia GPUs, or Qualcomm Snapdragons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fiftyone-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
