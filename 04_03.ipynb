{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViTs From Scratch\n",
    "### Chapter 4 Module 3\n",
    "\n",
    "Now we are going to go for gold and build it all from scratch. No helpers other than torch and torchvision, let's dive into building a transformer for computer vision ourselves!\n",
    "\n",
    "## Importing Libraries\n",
    "First things first, let's set up some config and import our libraries for our model. We will be referencing these variables throughout the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Basic configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device: \",device)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LR = 3e-4\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "PATCH_SIZE = 7  # 28 / 7 = 4x4 patches\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "EMBED_DIM = 64\n",
    "NUM_CLASSES = 10\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 2\n",
    "MLP_DIM = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MNIST Dataset\n",
    "This step downloads and prepares the MNIST dataset using torchvision.datasets. The images are transformed to tensors for model compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset MNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: .\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            ),\n",
       " Dataset MNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: .\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            ))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_data = datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root=\".\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_data,test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Patch Embedding Layer\n",
    "The Vision Transformer begins by splitting the image into fixed-size patches and embedding them into a higher-dimensional space. This class uses a convolutional layer to simulate that process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=1, patch_size=PATCH_SIZE, emb_dim=EMBED_DIM):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 1, 28, 28] → [B, emb_dim, 4, 4] → flatten to [B, 16, emb_dim]\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "Since transformers are permutation-invariant, this module adds learnable positional encodings to each patch to maintain spatial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_patches, emb_dim):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))  # +1 for [CLS] token\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder Block\n",
    "This defines a single encoder block consisting of multi-head self-attention and a feed-forward neural network, wrapped with layer normalization and residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.attn = nn.MultiheadAttention(emb_dim, num_heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention with skip connection\n",
    "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        # MLP with skip connection\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Vision Transformer (ViT) Model\n",
    "This is the core ViT architecture combining patch embedding, positional encoding, a stack of transformer blocks, and a final classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, EMBED_DIM))\n",
    "        self.pos_embed = PositionalEncoding(NUM_PATCHES, EMBED_DIM)\n",
    "        \n",
    "        self.encoder = nn.Sequential(*[\n",
    "            TransformerEncoderBlock(EMBED_DIM, NUM_HEADS, MLP_DIM)\n",
    "            for _ in range(NUM_LAYERS)\n",
    "        ])\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(EMBED_DIM),\n",
    "            nn.Linear(EMBED_DIM, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)  # [B, 16, EMBED_DIM]\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, EMBED_DIM]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # [B, 17, EMBED_DIM]\n",
    "        \n",
    "        x = self.pos_embed(x)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        cls_output = x[:, 0]  # Use the [CLS] token output\n",
    "        return self.head(cls_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "This loop handles the optimization of the ViT model on the MNIST training data using cross-entropy loss and Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Acc: 0.7957\n",
      "Epoch 2/5 - Train Acc: 0.9337\n",
      "Epoch 3/5 - Train Acc: 0.9514\n",
      "Epoch 4/5 - Train Acc: 0.9611\n",
      "Epoch 5/5 - Train Acc: 0.9655\n"
     ]
    }
   ],
   "source": [
    "model = ViT().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct += (preds.argmax(dim=1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train Acc: {train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Loop\n",
    "After training, this cell evaluates the model on the MNIST test set to calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9662\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total, correct = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        preds = model(imgs)\n",
    "\n",
    "        correct += (preds.argmax(dim=1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST into FiftyOne\n",
    "This step loads the MNIST test set into FiftyOne for visualization and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'test' to '/fiftyone/zoo/datasets/mnist/test'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 107MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 5.51MB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 36.1MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.17MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% ||------------|     0/10000 [576.4us elapsed, ? remaining, ? samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 10000/10000 [6.5s elapsed, 0s remaining, 2.0K samples/s]        \n",
      "Dataset info written to '/fiftyone/zoo/datasets/mnist/info.json'\n",
      "Loading existing dataset 'mnist-test'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://0.0.0.0:5151/?notebook=True&subscription=9216e2c1-6b51-495e-ae2a-6b1585ea1933&polling=true\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7edc3f4bbad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "#foz.delete_zoo_dataset(\"mnist\", split=\"test\")\n",
    "dataset = foz.load_zoo_dataset(\"mnist\", split=\"test\")\n",
    "\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Labels\n",
    "Here, the predicted labels are mapped to descriptive strings (e.g., “3 – three”) for better clarity in FiftyOne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://0.0.0.0:5151/?notebook=True&subscription=8a6b6cf0-583e-49a2-9cee-f517f1873814&polling=true\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7edc3d947610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "label_map = {0: \"0 - zero\", 1: \"1 - one\", 2: \"2 - two\", 3: \"3 - three\", 4: \"4 - four\", \n",
    "              5: \"5 - five\", 6: \"6 - six\", 7: \"7 - seven\", 8: \"8 - eight\", 9: \"9 - nine\"}\n",
    "\n",
    "for sample in dataset:\n",
    "    sample_tensor = transform(Image.open(sample.filepath).convert(\"L\")).unsqueeze(0)\n",
    "    sample_tensor = sample_tensor.to(device)\n",
    "    pred = model(sample_tensor).argmax(dim=1).item()\n",
    "    sample[\"ViT_prediction\"] = fo.Classification(label=label_map[pred])\n",
    "    sample.save()\n",
    "\n",
    "session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Predictions\n",
    "This evaluates the model's predictions using FiftyOne’s built-in classification metrics and prints a performance report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    0 - zero       0.98      0.98      0.98       980\n",
      "     1 - one       0.98      0.98      0.98      1135\n",
      "     2 - two       0.96      0.98      0.97      1032\n",
      "   3 - three       0.97      0.96      0.97      1010\n",
      "    4 - four       0.99      0.92      0.96       982\n",
      "    5 - five       0.97      0.96      0.96       892\n",
      "     6 - six       0.97      0.98      0.97       958\n",
      "   7 - seven       0.98      0.96      0.97      1028\n",
      "   8 - eight       0.93      0.98      0.95       974\n",
      "    9 - nine       0.94      0.96      0.95      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = dataset.evaluate_classifications(ground_truth=\"ground_truth\", pred_field=\"ViT_prediction\", eval_key=\"eval\")\n",
    "results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Misclassifications\n",
    "This creates a filtered view of only the incorrectly predicted samples for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:     mnist-test\n",
      "Media type:  image\n",
      "Num samples: 340\n",
      "Sample fields:\n",
      "    id:               fiftyone.core.fields.ObjectIdField\n",
      "    filepath:         fiftyone.core.fields.StringField\n",
      "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    created_at:       fiftyone.core.fields.DateTimeField\n",
      "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
      "    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
      "    ViT_prediction:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
      "    eval:             fiftyone.core.fields.BooleanField\n",
      "View stages:\n",
      "    1. Match(filter={'$expr': {'$eq': [...]}})\n"
     ]
    }
   ],
   "source": [
    "from fiftyone import ViewField as F\n",
    "view = dataset.match(F(\"eval\")==False)\n",
    "print(view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://0.0.0.0:5151/?notebook=True&subscription=76c4d62d-1d35-4f6f-8dfe-3db2ae894bbc&polling=true\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7edc3f967e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session.view = view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Eval Panel!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fiftyone-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
