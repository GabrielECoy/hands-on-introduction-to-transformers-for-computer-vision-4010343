{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing CNN to Transformer\n",
    "## Chapter 2 Module 4\n",
    "\n",
    "In our first code example, we are going to install some key libraries that we will need for our code practices later. We will also do a very basic comparision of the two famous models, AlexNet vs ViT!\n",
    "\n",
    "## Installation\n",
    "Before we can get started, pip install the libraries below. `torch` `torchvision` `transformers` `pillow` will all help with our model inference. `fiftyone` is an open source library to help manage computer vision datasets as well as evaluation, visualization, and more that we will leverage for our examples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision transformers pillow  fiftyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Subset of Imagenet\n",
    "\n",
    "The first part of FiftyOne we will leverage is the dataset zoo. Instead of manually collecting all 1TB of [ImageNet](https://www.image-net.org/index.php), we will use a subset from the [FiftyOne Dataset Zoo](https://docs.voxel51.com/dataset_zoo/index.html) called [ImageNet-Sample](https://docs.voxel51.com/dataset_zoo/datasets.html#imagenet-sample). The sample contains 1,000 images but we will load in just 50 for our example.\n",
    "\n",
    "After the dataset is loaded, we launch the [FiftyOne App](https://docs.voxel51.com/user_guide/app.html) to visualize our dataset. You can view the app in your notebook below the cell. If you are running locally, you can also launch the notebook in your browser with `localhost:5151`. Try exploring ImageNet for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.fiftyone-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset to '/fiftyone/zoo/datasets/imagenet-sample'\n",
      "Downloading dataset...\n",
      " 100% |████|  762.4Mb/762.4Mb [1.2s elapsed, 0s remaining, 664.3Mb/s]         \n",
      "Extracting dataset...\n",
      "Parsing dataset metadata\n",
      "Found 1000 samples\n",
      "Dataset info written to '/fiftyone/zoo/datasets/imagenet-sample/info.json'\n",
      "Loading 'imagenet-sample'\n",
      " 100% |███████████████████| 50/50 [77.8ms elapsed, 0s remaining, 642.4 samples/s]  \n",
      "Dataset 'Imagenet-Sample' created\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz \n",
    "\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"imagenet-sample\",\n",
    "    dataset_name=\"Imagenet-Sample\",\n",
    "    max_samples=50,\n",
    "    shuffle=True,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://0.0.0.0:5151/?notebook=True&subscription=f7a64ab9-4b87-41d0-b756-15af5d0ae128&polling=true\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x77e56f310690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenet](./assets/imagenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try filtering your dataset on its labels by using the sidebar! You can select any of the classes in your dataset to see those samples!\n",
    "\n",
    "![labels](./assets/labels.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Showdown\n",
    "\n",
    "Now that we have our dataset loaded and ready to go, its time to test our two models against eachother! \n",
    "\n",
    "We will be using the [Google ViT](https://huggingface.co/google/vit-base-patch16-224) from the [Hugging Face](https://huggingface.co/) [`transformers` library](https://huggingface.co/docs/transformers/en/index). We will be using this library often so feel free to get familiar!\n",
    "\n",
    "The AlexNet model in the [FiftyOne Model Zoo](https://docs.voxel51.com/model_zoo/models.html) comes from [pytorch models](https://pytorch.org/vision/main/models.html) and is based off the original paper.\n",
    "\n",
    "We use [FiftyOne's Hugging Face integration](https://docs.voxel51.com/integrations/huggingface.html) to take any `transformers` model and apply it instantly to our dataset. We can do the same with any model from the FiftyOne Model Zoo. After inference is done, check back in your app and compare performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 50/50 [28.1s elapsed, 0s remaining, 2.0 samples/s]      \n",
      "Downloading model from 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth'...\n",
      " 100% |██████|    1.8Gb/1.8Gb [7.0s elapsed, 0s remaining, 120.2Mb/s]      \n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233M/233M [00:01<00:00, 177MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 50/50 [18.7s elapsed, 0s remaining, 4.4 samples/s]      \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://0.0.0.0:5151/?notebook=True&subscription=3e8d2650-bacf-40eb-a879-b0461436a1e4&polling=true\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x77e535c32990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\"\n",
    ")\n",
    "dataset.apply_model(model, label_field=\"ViT_predictions\")\n",
    "\n",
    "model = foz.load_zoo_model(\"alexnet-imagenet-torch\")\n",
    "dataset.apply_model(model, label_field=\"AlexNet-predictions\")\n",
    "\n",
    "\n",
    "session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare](./assets/compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking Forward\n",
    "\n",
    "In our next chapter, we will learn all about how we can begin using transformers right away, including how to load, inference, and evaluate transformers yourself. Hop over to Chapter 3 to learn more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fiftyone-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
