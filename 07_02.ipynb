{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Attention\n",
    "### Chapter 7 Module 2\n",
    "\n",
    "## Importing Libraries\n",
    "We begin by importing the necessary libraries for loading a pretrained Vision Transformer, processing images, and using FiftyOne to manage and visualize datasets and attention maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.fiftyone-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTModel, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import fiftyone as fo \n",
    "import fiftyone.zoo as foz\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading an ImageNet Sample Dataset\n",
    "We load a small subset of ImageNet via FiftyOne’s dataset zoo. This gives us real-world image samples to test and visualize attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing directory '/fiftyone/zoo/datasets/imagenet-sample'\n",
      "Downloading dataset to '/fiftyone/zoo/datasets/imagenet-sample'\n",
      "Downloading dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |████|  762.4Mb/762.4Mb [917.2ms elapsed, 0s remaining, 831.2Mb/s]      \n",
      "Extracting dataset...\n",
      "Parsing dataset metadata\n",
      "Found 1000 samples\n",
      "Dataset info written to '/fiftyone/zoo/datasets/imagenet-sample/info.json'\n",
      "Loading 'imagenet-sample'\n",
      " 100% |███████████████████| 50/50 [44.9ms elapsed, 0s remaining, 1.1K samples/s]      \n",
      "Dataset 'Imagenet-Sample' created\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://0.0.0.0:5151/?notebook=True&subscription=6b60c684-5919-4271-863a-eeb5162c5bd0&polling=true\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x71302f5b5150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fo.delete_dataset(\"Imagenet-Sample\")\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"imagenet-sample\",\n",
    "    dataset_name=\"Imagenet-Sample\",\n",
    "    max_samples=50,\n",
    "    shuffle=True,\n",
    "    overwrite=True,\n",
    ")\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Vision Transformer Model\n",
    "We load a pretrained ViT model from Hugging Face with output_attentions=True to retrieve attention weights from each layer. The corresponding image processor is also initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "model = ViTModel.from_pretrained(model_name, output_attentions=True)\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing a Single Image\n",
    "Here we select the first image in the dataset and convert it into the appropriate input tensor using the Hugging Face image processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(dataset.first().filepath)\n",
    "inputs = processor(images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference and Extracting Attention\n",
    "We pass the image through the model and collect the attention weights from all transformer layers. These attention maps will be used for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    attentions = outputs.attentions  # list of (B, heads, tokens, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting and Averaging the Last Layer's CLS Attention\n",
    "We extract the last layer's attention for the [CLS] token across all heads and average them to get a single attention map highlighting where the model focused when forming the global representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use last layer attention\n",
    "last_attn = attentions[-1]  # shape: (1, num_heads, tokens, tokens)\n",
    "cls_attn = last_attn[0, :, 0, 1:]  # shape: (num_heads, num_patches)\n",
    "cls_attn = cls_attn.mean(0)  # mean over heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the Attention Map\n",
    "This reshapes the attention vector into a 2D grid corresponding to the 14x14 patch layout of a 224×224 image split into 16×16 patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For vit-base-patch16-224 → 224/16 = 14 → 14x14 patches\n",
    "cls_attn_map = cls_attn.reshape(14, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling the Attention Map to Image Size\n",
    "The 14×14 attention map is upsampled to 224×224 using bilinear interpolation to match the original image size, making it suitable for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Upsample to image size\n",
    "attn_map_resized = F.interpolate(\n",
    "    cls_attn_map.unsqueeze(0).unsqueeze(0),  # (1,1,H,W)\n",
    "    size=(224, 224),\n",
    "    mode=\"bilinear\"\n",
    ").squeeze().numpy()\n",
    "\n",
    "attn_map_resized = (attn_map_resized - attn_map_resized.min()) / (attn_map_resized.max() - attn_map_resized.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_map_resized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Additional Visualization Libraries\n",
    "Here we import OpenCV, NumPy, and Matplotlib to support reading, manipulating, and displaying attention maps as heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_maps/688f9baa5973cb0f0b98b763.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(\"attn_maps\", exist_ok=True)\n",
    "\n",
    "# Normalize the attention map to 0-255 and save as a single-channel (grayscale) PNG\n",
    "attn_map_resized_uint8 = (attn_map_resized * 255).astype(np.uint8)\n",
    "cv2.imwrite(f\"attn_maps/{dataset.first().id}.png\", attn_map_resized_uint8)\n",
    "\n",
    "print(f\"attn_maps/{dataset.first().id}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Displaying the Attention Map\n",
    "The attention heatmap is saved to disk and added to the FiftyOne sample view, allowing interactive exploration inside the FiftyOne App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://0.0.0.0:5151/?notebook=True&subscription=1905d742-3927-4838-9891-88efb0500ebc&polling=true\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7130175ec610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = dataset.first()\n",
    "sample[\"attention\"] = fo.Heatmap(map_path=f\"/Users/dangural/Documents/linkedin/tcv/ch7/attn_maps/{dataset.first().id}.png\")\n",
    "sample.save()\n",
    "session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Attention Rollout\n",
    "This function implements attention rollout, a technique that accumulates attention across layers to show how attention flows from input patches to the [CLS] token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rollout(attentions, add_residual=True, res_weight=0.1):\n",
    "    \"\"\"\n",
    "    Compute attention rollout from a list of attention matrices.\n",
    "    - attentions: list of tensors, each of shape (batch, heads, tokens, tokens)\n",
    "    - returns: rollout attention for [CLS] token\n",
    "    \"\"\"\n",
    "    num_tokens = attentions[0].size(-1)\n",
    "    rollout = torch.eye(num_tokens).to(attentions[0].device)\n",
    "\n",
    "    for attn in attentions:\n",
    "        avg_attn = attn[0].mean(0)  # average over heads\n",
    "        \n",
    "        if add_residual:\n",
    "            avg_attn = res_weight * torch.eye(num_tokens).to(attn.device) + (1 - res_weight) * avg_attn\n",
    "\n",
    "        avg_attn = avg_attn / avg_attn.sum(dim=-1, keepdim=True)  # normalize\n",
    "        rollout = avg_attn @ rollout\n",
    "\n",
    "    return rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Rollout Attention Maps\n",
    "This applies the attention rollout method and reshapes the resulting map into a 14×14 grid for visualization, just like with the last-layer attention map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_attn = compute_rollout(attentions)\n",
    "cls_to_patch = rollout_attn[0, 1:]  # (excluding CLS itself)\n",
    "cls_map = cls_to_patch.reshape(14, 14)  # for 224x224 input with patch size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling Rollout Map\n",
    "Similar to earlier, we upsample the rollout attention to 224×224 pixels for better overlay on the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upsample to image size\n",
    "attn_map_resized = F.interpolate(\n",
    "    cls_map.unsqueeze(0).unsqueeze(0),  # (1,1,H,W)\n",
    "    size=(224, 224),\n",
    "    mode=\"bilinear\"\n",
    ").squeeze().numpy()\n",
    "\n",
    "attn_map_resized = (attn_map_resized - attn_map_resized.min()) / (attn_map_resized.max() - attn_map_resized.min())\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(\"attn_maps\", exist_ok=True)\n",
    "\n",
    "# Normalize the attention map to 0-255 and save as a single-channel (grayscale) PNG\n",
    "attn_map_resized_uint8 = (attn_map_resized * 255).astype(np.uint8)\n",
    "cv2.imwrite(f\"attn_maps/{dataset.first().id}_rollout.png\", attn_map_resized_uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Rollout Attention in FiftyOne\n",
    "The rollout attention map is saved and attached to the sample in FiftyOne, allowing side-by-side comparison with the last-layer attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://0.0.0.0:5151/?notebook=True&subscription=5846ad12-177e-4452-be29-1e4153c22114&polling=true\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x71302c0f4ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = dataset.first()\n",
    "sample[\"rollout_attention\"] = fo.Heatmap(map_path=f\"/Users/dangural/Documents/linkedin/tcv/ch7/attn_maps/{dataset.first().id}_rollout.png\")\n",
    "sample.save()\n",
    "session.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fiftyone-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
